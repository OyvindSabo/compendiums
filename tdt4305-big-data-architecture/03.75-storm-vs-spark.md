# Storm vs Spark

|                     | Storm                    | Spark                       |
|---------------------|--------------------------|-----------------------------|
|Processing Model     |Event-Streaming     |Micro-Batching / Batch (Spark Core)|
|Delivery Guarantees|At most once / At least once|Exactly Once                 |
|Latency              |Sub-second                |Seconds                      |
|Language Options|Java, Clojure, Scala, Python, Ruby|Java, Scala, Python       |
|Development  |Use other tool for batch|Batching and streaming are very similar|

## When to use Spark
- Iterative Batch Processing (most Machine Learning)
- ETL - Extract, Transform and Load
- Shark/Interactive Queries
- < 1 TB (or memory size of your cluster)
- Tuning to run well can be a pain
- Data Bricks and others are working on scaling.
- Streaming all µ-batch:
- latency at least 1 sec
- Still single points of failure in streaming
- All streaming inputs replicated in memory

## When to use Storm
- Latency < 1 second (single event at a time)
- There is little else (especially not open source)
- “Real Time” -Analytics -Budgeting -ML -Anything
- Lower Level API than Spark
- No built-in concept of look back aggregations
- Takes more effort to combine batch with streaming

## Storm vs Spark 2

| Storm                                | Spark                                 |
|--------------------------------------|---------------------------------------|
| Tends to be driven by creating classes and implementing interfaces | Has more of a “functional” flavor, where working with the API is driven more by chaining successive method calls to invoke primitive operations                         |
| Advantage of broader language support (code written in R or any other language not natively supported by Spark) | Tuples can feel awkward in Java but with this is going benefit of compile-time                                     |
| DAG’s as natural processing model Tuple natural interface for the data passed between nodes | Use existing Hadoop or Mesos cluster                           |
| Processing excels at computing transformations as data are ingested with sub-second latencies | Micro-batching trivially gives stateful computation, making windowing an easy task.                                                 |

## When is Spark not a good fit?
If your goal is to create dashboards that provide monitoring and insights based on a data stream. Spark is a weaker fit for this use case, both because its ad-hoc nature isn’t very useful for building production systems, and because it doesn’t support streaming data well. This is often a case where Spark is used in conjunction with a few other tools to manage the constant flow of data.